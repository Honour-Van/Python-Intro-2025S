import os
import warnings
from typing import List

from langchain.docstore.document import Document
from langchain_community.document_loaders import (
    PyPDFLoader, TextLoader, Docx2txtLoader, 
    UnstructuredPowerPointLoader, CSVLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.output_parsers import RegexParser
from langchain.schema.runnable import RunnableLambda


# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", message="Ignoring wrong pointing object")
# warnings.filterwarnings("ignore", category=DeprecationWarning, module="langchain")

class DocumentProcessor:
    SUPPORTED_EXTS = {
        '.pdf': PyPDFLoader,
        '.txt': lambda p: TextLoader(p, encoding='utf-8'),
        '.docx': Docx2txtLoader,
        '.doc': Docx2txtLoader,
        '.pptx': UnstructuredPowerPointLoader,
        '.csv': CSVLoader
    }

    @classmethod
    def load_document(cls, file_path: str) -> List[Document]:
        ext = os.path.splitext(file_path)[1].lower()
        if ext not in cls.SUPPORTED_EXTS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")
        try:
            loader = cls.SUPPORTED_EXTS[ext](file_path)
            return loader.load()
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ–‡ä»¶å¤±è´¥: {file_path}") from e

    @classmethod
    def load_documents_from_dir(cls, directory: str) -> List[Document]:
        all_docs = []
        for root, _, files in os.walk(directory):
            for file in files:
                ext = os.path.splitext(file)[1].lower()
                if ext in cls.SUPPORTED_EXTS:
                    file_path = os.path.join(root, file)
                    try:
                        print(f"æ­£åœ¨åŠ è½½: {file_path}")
                        docs = cls.load_document(file_path)
                        all_docs.extend(docs)
                        print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} é¡µ")
                    except Exception as e:
                        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return all_docs


class RAGEngine:
    DEFAULT_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç§é—®é¢˜ã€‚
ä½ éœ€è¦ä¾æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¿”å›â€œæœªæ‰¾åˆ°å‚è€ƒæ–‡æ¡£â€ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

ç”¨æˆ·é—®é¢˜:
{input}

å›ç­”:
"""

    FALLBACK_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç§é—®é¢˜ã€‚
ç›®å‰æ²¡æœ‰æ‰¾åˆ°ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å…·ä½“ä¿¡æ¯ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚

ç”¨æˆ·é—®é¢˜:
{question}

å›ç­”:
"""

    def __init__(self, embedding_model="nomic-embed-text", llm_model="deepseek-r1:7b"):
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        self.vectorstore = None
        self.qa_chain = None
        self.fallback_chain = None
        self.parser = None

    def process_documents(self, documents: List[Document]):
        if not documents:
            raise ValueError("æ–‡æ¡£ä¸ºç©ºï¼Œæ— æ³•å¤„ç†")

        print("\nğŸ“– æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(documents)

        print(f"ğŸ”¢ å…±ç”Ÿæˆ {len(texts)} ä¸ªæ–‡æœ¬å—ï¼Œå¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
        embeddings = OllamaEmbeddings(
            # base_url="http://localhost:11434", # windowsè¦åŠ è¿™ä¸ªé€‰é¡¹ï¼Œmacä¸åŠ 
            model=self.embedding_model)
        self.vectorstore = Chroma.from_documents(texts, embeddings)
        print("âœ… å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ")

    def init_output_parser(self):
        # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼è§£æå™¨ï¼Œæå–å†’å·åçš„å†…å®¹
        # ä¸ºä»€ä¹ˆä¸å¯¹ï¼Ÿpattern = r"<think>(.*?)</think>\s*(.*)" # é»˜è®¤ä¸å¼€DOTALL
        deepseek_pattern = r"<think>([\s\S]*?)</think>\s*([\s\S]*)"
        output_keys = ["thought", "answer"]

        self.parser = RegexParser(
            regex=deepseek_pattern,
            output_keys=output_keys,
            default_output_key="answer",
        )


    def create_qa_chain(self):
        if self.vectorstore is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ process_documents åˆå§‹åŒ–å‘é‡å­˜å‚¨")

        llm = OllamaLLM(
            # base_url="http://localhost:11434", # windowsè¦åŠ è¿™ä¸ªé€‰é¡¹ï¼Œmacä¸åŠ 
            model=self.llm_model)

        retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 3}
        )

        # æ–°ç‰ˆPrompt + LLMæ£€ç´¢é“¾
        primary_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", self.DEFAULT_PROMPT),
                    ("human", "{input}"),
                ]
            )
        document_chain = create_stuff_documents_chain(llm, primary_prompt)
        self.qa_chain = create_retrieval_chain(retriever, document_chain).pick("answer") | self.parser

        fallback_prompt = PromptTemplate(
            template=self.FALLBACK_PROMPT,
            input_variables=["question"]
        )
        self.fallback_chain = fallback_prompt | llm

    def ask(self, question: str) -> str:
        if self.qa_chain is None or self.fallback_chain is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ create_qa_chain() åˆå§‹åŒ–é—®ç­”é“¾")

        try:
            result = self.qa_chain.invoke({"input": question})
            # print(result) # ä¸çŸ¥é“è°ƒç”¨è¾“å…¥è¾“å‡ºç»“æ„ï¼Œå°±æ‰“å°ï¼Œåƒåšé¢˜ä¸€æ ·
            answer = result['answer']
            if not answer.strip():
                raise ValueError("ç­”æ¡ˆä¸ºç©º")
            if result.get("context"):
                print("\nğŸ“š å‚è€ƒæ–‡æ¡£:")
                for i, doc in enumerate(result["context"]):
                    print(f"{i}. {os.path.basename(doc.metadata.get('source', 'æœªçŸ¥æ–‡æ¡£'))}")

            if "æœªæ‰¾åˆ°å‚è€ƒæ–‡æ¡£" in answer:
                raise ValueError("æœªæ‰¾åˆ°å‚è€ƒæ–‡æ¡£ï¼Œè°ƒç”¨fallbacké“¾")
            return answer
        except Exception as e:
            print(f"âš ï¸ æ£€ç´¢å¤±è´¥æˆ–æ— ç»“æœï¼Œä½¿ç”¨ fallback: {e}")
            # return self.fallback_chain.invoke({"question": question})

def main():
    """ä¸»å‡½æ•°"""
    print("\n===== æ–‡æ¡£æ™ºèƒ½é—®ç­”ç³»ç»Ÿ =====")
    
    # è·å–æ–‡æ¡£è·¯å¾„
    while True:
        path = input("\nè¯·è¾“å…¥æ–‡æ¡£è·¯å¾„æˆ–ç›®å½• (ä¾‹å¦‚: ./docs): ").strip()
        if not path:
            print("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue
            
        if os.path.isfile(path) or os.path.isdir(path):
            break
        else:
            print("è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    # åŠ è½½æ–‡æ¡£
    docs = DocumentProcessor.load_documents_from_dir(path)
    
    # åˆå§‹åŒ–RAGå¼•æ“
    engine = RAGEngine()
    
    try:
        if docs:  # åªæœ‰å½“æœ‰æ–‡æ¡£æ—¶æ‰å¤„ç†
            engine.process_documents(docs)
        engine.init_output_parser()
        engine.create_qa_chain()
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return
    
    # äº¤äº’å¼é—®ç­”
    print("\n===== å¼€å§‹äº¤äº’å¼é—®ç­” =====")
    print("è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢ï¼Œè¾“å…¥ 'q' é€€å‡º")
    
    try:
        while True:
            query = input("\nâ“ è¯·è¾“å…¥é—®é¢˜ï¼ˆæˆ–è¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š\n> ").strip()
            if query.strip().lower() == "exit":
                break
            if not query:
                continue
                
            try:
                answer = engine.ask(query)
                print(answer)
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
    except KeyboardInterrupt:
        print("")
    finally:
        print("\næ„Ÿè°¢ä½¿ç”¨æ–‡æ¡£æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼")


if __name__ == "__main__":
    main()

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f033a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
      "  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.41-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/0508/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/0508/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.18-cp313-cp313-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/0508/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/0508/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/0508/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/0508/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/0508/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Downloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading orjson-3.10.18-cp313-cp313-macosx_15_0_arm64.whl (133 kB)\n",
      "Using cached pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.4/633.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, SQLAlchemy, PyYAML, pydantic-core, orjson, jsonpointer, httpcore, charset-normalizer, anyio, annotated-types, requests, pydantic, jsonpatch, httpx, requests-toolbelt, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [langchain]21\u001b[0m [langchain]core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyYAML-6.0.2 SQLAlchemy-2.0.41 annotated-types-0.7.0 anyio-4.9.0 charset-normalizer-3.4.2 httpcore-1.0.9 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.25 langchain-core-0.3.60 langchain-text-splitters-0.3.8 langsmith-0.3.42 orjson-3.10.18 pydantic-2.11.4 pydantic-core-2.33.2 requests-2.32.3 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspection-0.4.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f3fcb",
   "metadata": {},
   "source": [
    "## 组件\n",
    "\n",
    "### Prompt Templates\n",
    "\n",
    "Prompt Templates （提示词模板）​用于创建动态提示，可根据用户输入进行定制。它就像是一个 “填空模板”，定义好输入变量和模板内容，运行时将具体值填入变量位置生成完整提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0286b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Alice! Welcome to LangChain.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "# 定义提示模板，input_variables指定输入变量，template为模板内容\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"Hello, {name}! Welcome to LangChain.\"\n",
    ")\n",
    "# 使用format方法传入具体值生成提示\n",
    "output = prompt.format(name=\"Alice\")\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab378f8",
   "metadata": {},
   "source": [
    "### Models（模型）\n",
    "\n",
    "LangChain 支持多种模型，可轻松集成到应用中。如使用 OpenAI 的 GPT 模型，代码如下：\n",
    "\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "# 初始化OpenAI模型，需设置API Key\n",
    "llm = OpenAI(api_key=\"your_openai_api_key\")\n",
    "# 使用模型生成文本\n",
    "response = llm(\"Tell me a joke.\")\n",
    "print(response) \n",
    "```\n",
    "\n",
    "本地部署的ollama模型可以使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624f65c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "llm = OllamaLLM(\n",
    "            # base_url=\"http://localhost:11434\", # windows要加这个选项，mac不加\n",
    "            model=\"deepseek-r1:7b\")\n",
    "llm.invoke(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cff3e5",
   "metadata": {},
   "source": [
    "### Output Parsers（输出解析器）​\n",
    "Output Parsers 帮助处理模型输出，使其更符合应用使用需求。例如，模型输出可能是原始文本，输出解析器可将其解析为结构化数据（如 JSON 格式）。​\n",
    "\n",
    "DeepSeek R1会有思考过程，问题简单时打印的结果think和answer高度重复，我们可以自行过滤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5803425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: \n",
      "\n",
      "\n",
      "Answer: 您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.output_parsers import RegexParser\n",
    "\n",
    "llm = OllamaLLM(\n",
    "            # base_url=\"http://localhost:11434\", # windows要加这个选项，mac不加\n",
    "            model=\"deepseek-r1:7b\")\n",
    "raw_output = llm.invoke(\"你是谁\")\n",
    "\n",
    "# 定义正则表达式解析器，提取冒号后的内容\n",
    "# 为什么不对？pattern = r\"<think>(.*?)</think>\\s*(.*)\" # 默认不开DOTALL\n",
    "pattern = r\"<think>([\\s\\S]*?)</think>\\s*([\\s\\S]*)\"\n",
    "output_keys = [\"thought\", \"answer\"]\n",
    "\n",
    "parser = RegexParser(\n",
    "    regex=pattern,\n",
    "    output_keys=output_keys,\n",
    "    default_output_key=\"answer\",\n",
    ")\n",
    "parsed = parser.parse(raw_output)\n",
    "\n",
    "print(\"Thought:\", parsed[\"thought\"])\n",
    "print(\"Answer:\", parsed[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ba18d",
   "metadata": {},
   "source": [
    "## 链\n",
    "\n",
    "Chain 用于组合多个组件，完成更复杂的任务。例如将提示模板和模型组合成一个链，根据输入生成输出。​\n",
    "\n",
    "LangChain 使用了「管道操作符」|（pipe operator），让多个组件组合起来更直观。\n",
    "\n",
    "在内部，其实是通过实现了 `__or__()` 魔术方法（Python 的 | 运算符），将组件自动连接成一个链。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c732aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n好，我现在需要处理用户发来的这条消息：“你好，我是Alice，请以我的名字呼唤我，并告诉我你是谁”。首先，我要理解用户的请求是什么。看起来用户希望我用“Alice”作为问候的称呼，并且明确说明自己是Alice。\\n\\n接下来，我要确认我的角色和职责。作为一个智能助手，我需要保持友好、专业并且准确。所以，在回应时，我会开头以“你好，我是Alice”来称呼对方，这样既符合用户的请求，又显得礼貌。\\n\\n然后，我需要详细解释一下我是谁，包括我的功能、用途以及未来提供的服务，比如信息查询、学习建议等。这样用户能更好地了解我的作用和提供的帮助范围。\\n\\n同时，我要确保语言简洁明了，避免使用复杂的术语或过多的说明，让用户能够轻松理解。此外，语气要友好，让用户感到被重视和支持。\\n\\n最后，我需要总结一下，确认用户的请求已经被满足，并邀请用户提供更多信息或者提出具体的问题，这样可以进一步促进互动。\\n\\n在实际操作中，可能会遇到用户没有明确表达的需求，比如是否有特定的话题想探讨或是否需要帮助解决某个问题。因此，在回应中要保持开放的态度，以便根据后续的交流调整内容和提供更有针对性的帮助。\\n</think>\\n\\n你好，我是Alice，一个由中国的深度求索（DeepSeek）公司开发的智能助手。我擅长通过思考来帮您解答复杂的数学、代码和逻辑推理等理工类问题，也可以辅助您进行信息查询。请问有什么可以帮到您的？'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 定义提示模板，input_variables指定输入变量，template为模板内容\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"你好，我是{name}，请以我的名字呼唤我，并告诉我你是谁\"\n",
    ")\n",
    "\n",
    "llm = OllamaLLM(\n",
    "            # base_url=\"http://localhost:11434\", # windows要加这个选项，mac不加\n",
    "            model=\"deepseek-r1:7b\")\n",
    "chain = prompt | llm\n",
    "# chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain.invoke({\"name\":\"Alice\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013eb0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "好，用户发来了“你好”，我应该用中文回应他。首先，我需要礼貌地回应他的问候，然后询问他的具体需求是什么。这样可以让对话继续下去，并且帮助我更好地理解用户的需求。\n",
      "\n",
      "在回应的时候，要保持友好和亲切，避免使用太正式的语言，让对话感觉自然。接着，提出一个开放性的问题，让用户进一步说明他需要什么帮助。比如问：“有什么我可以为您效劳的？”或者类似的表达。\n",
      "\n",
      "这样既表达了关心，又引导用户明确他们的需求，确保后续的对话能够顺利进行。\n",
      "</think>\n",
      "\n",
      "你好！有什么我可以为您效劳的？\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# 1. 构造 prompt 和 LLM\n",
    "prompt = PromptTemplate.from_template(\"你是一个助手。\\n历史记录:\\n{chat_history}\\n\\n用户: {input}\\n助手:\")\n",
    "llm = OllamaLLM(\n",
    "            # base_url=\"http://localhost:11434\", # windows要加这个选项，mac不加\n",
    "            model=\"deepseek-r1:7b\")\n",
    "\n",
    "# 2. 组合成一个 LLMChain\n",
    "chain = prompt | llm\n",
    "\n",
    "# 3. 用 message history 包装\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: InMemoryChatMessageHistory(),  # 每个 session 都有自己的历史\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# 4. 使用方式\n",
    "session_id = \"student-session-1\"\n",
    "response = conversation.invoke({\"input\": \"你好\"}, config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53678813",
   "metadata": {},
   "source": [
    "RetrievalQA是langchain中专门支持RAG的chain类型，也将弃用。但其语义相对清晰，也予以讲解。\n",
    "```python\n",
    "RetrievalQA.from_chain_type( # 使用旧接口\n",
    "            llm,\n",
    "            retriever=vectorstore.as_retriever(),\n",
    "            chain_type=\"stuff\"\n",
    "        )\n",
    "```\n",
    "\n",
    "\n",
    "较新的接口：\n",
    "```\n",
    "document_chain = create_stuff_documents_chain(llm, primary_prompt)\n",
    "qa_chain = create_retrieval_chain(retriever, document_chain)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f3dc4",
   "metadata": {},
   "source": [
    "### 文件加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c70f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "import os\n",
    "\n",
    "SUPPORTED_EXTS = {\n",
    "    '.txt': lambda p: TextLoader(p, encoding='utf-8'),\n",
    "    '.pdf': PyPDFLoader\n",
    "}\n",
    "file_path = './docs/考试时间.txt'\n",
    "\n",
    "ext = os.path.splitext(file_path)[1].lower()\n",
    "if ext not in SUPPORTED_EXTS:\n",
    "    raise ValueError(f\"不支持的文件格式: {ext}\")\n",
    "try:\n",
    "    loader = SUPPORTED_EXTS[ext](file_path)\n",
    "    doc = loader.load()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"加载文件失败: {file_path}\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc4101",
   "metadata": {},
   "source": [
    "### 向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d016a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "            # base_url=\"http://localhost:11434\", # windows要加这个选项，mac不加\n",
    "            model='nomic-embed-text')\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(doc)\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6263859",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # or \"similarity\"\n",
    "    search_kwargs={\n",
    "        \"k\": 5,                 # 返回前k个相似文档\n",
    "        \"score_threshold\": 0.7, # 相似度分数阈值（可选，部分vectorstore支持）\n",
    "        \"fetch_k\": 20,          # 用于MMR时：从top-N中选择（比如先取前20个再挑5个）\n",
    "        \"lambda_mult\": 0.5      # MMR中控制多样性和平衡性的超参数，score = λ * 新文档相似度 - (1 - λ) * 已选文档相似度，越高就找越相关的\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73d20e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '你好，你是谁',\n",
       " 'context': [Document(metadata={'source': './docs/考试时间.txt'}, page_content='课程考试在第15周或者第16周'),\n",
       "  Document(metadata={'source': './docs/考试时间.txt'}, page_content='课程考试在第15周或者第16周')],\n",
       " 'answer': '<think>\\n好，我现在需要解决用户的查询。用户首先问：“你好，你是谁。” 这是一个常见的问候问题，通常用于自我介绍。\\n\\n查看上下文信息，发现没有关于课程考试的详细内容，只有时间安排的信息，并且重复了两次，说明可能只是强调考试的时间在第15或16周。\\n\\n由于问题主要询问我的身份，而不是课程考试相关的内容，所以我应该专注于回答用户的问题。因此，我会简单地回复“你好！我是智能助手。” 这样既回答了问候，又表明了我的功能。\\n</think>\\n\\n你好！我是智能助手。'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "DEFAULT_PROMPT = \"\"\"\n",
    "你是一个智能助手，擅长回答各种问题。\n",
    "你需要依据以下提供的上下文信息回答用户的问题。\n",
    "如果上下文信息不足，返回“未找到参考文档”。\n",
    "\n",
    "上下文信息:\n",
    "{context}\n",
    "\n",
    "用户问题:\n",
    "{input}\n",
    "\n",
    "回答:\n",
    "\"\"\"\n",
    "\n",
    "primary_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", DEFAULT_PROMPT),\n",
    "                    (\"human\", \"{input}\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, primary_prompt)\n",
    "qa_chain = create_retrieval_chain(retriever, document_chain)\n",
    "question = \"你好，你是谁\"\n",
    "qa_chain.invoke({\"input\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
